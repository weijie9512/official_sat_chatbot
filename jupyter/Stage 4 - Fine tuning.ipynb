{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"b-QO6K-nf1zt","executionInfo":{"status":"ok","timestamp":1661633202194,"user_tz":-60,"elapsed":20226,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["  %%capture\n","!pip install datasets\n","!pip install transformers"],"id":"b-QO6K-nf1zt"},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2982,"status":"ok","timestamp":1661633205170,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"4a9d7266"},"outputs":[],"source":["# cite simcse\n","# cite sentence-transformers https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","import torch\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"],"id":"4a9d7266"},{"cell_type":"markdown","metadata":{"id":"a79c2df2"},"source":["## My Dataset"],"id":"a79c2df2"},{"cell_type":"code","execution_count":101,"metadata":{"id":"gNSlFnNYfOLO","executionInfo":{"status":"ok","timestamp":1661635417720,"user_tz":-60,"elapsed":424,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["colab = True"],"id":"gNSlFnNYfOLO"},{"cell_type":"code","execution_count":102,"metadata":{"id":"397e14e8","executionInfo":{"status":"ok","timestamp":1661635418598,"user_tz":-60,"elapsed":4,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["label = \"final_pred_modified\""],"id":"397e14e8"},{"cell_type":"code","execution_count":103,"metadata":{"id":"239e25a8","executionInfo":{"status":"ok","timestamp":1661635420164,"user_tz":-60,"elapsed":1011,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report"],"id":"239e25a8"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNyLRm3VDtU7","executionInfo":{"status":"ok","timestamp":1661635422226,"user_tz":-60,"elapsed":2066,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}},"outputId":"1776d760-6180-4e44-a269-a0dd7b460988"},"id":"VNyLRm3VDtU7","execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"KY9U5tyMD10s","executionInfo":{"status":"ok","timestamp":1661635422226,"user_tz":-60,"elapsed":3,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"id":"KY9U5tyMD10s","execution_count":104,"outputs":[]},{"cell_type":"code","execution_count":105,"metadata":{"id":"70442d93","executionInfo":{"status":"ok","timestamp":1661635422226,"user_tz":-60,"elapsed":3,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["#df = pd.read_excel(\"data/survey_data/labelled_data/sat_data_combined_vertical_pred_final.xlsx\")\n","if colab:\n","  df = pd.read_excel(\"/content/drive/MyDrive/Imperial College London/Individual Project/Jupyter/data_labelled.xlsx\")\n","else:\n","  df = pd.read_excel(\"data/survey_data/labelled_data/data_labelled.xlsx\")"],"id":"70442d93"},{"cell_type":"code","execution_count":106,"metadata":{"id":"ea405aa6","executionInfo":{"status":"ok","timestamp":1661635422226,"user_tz":-60,"elapsed":2,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["df[label] = df[label].astype(int)"],"id":"ea405aa6"},{"cell_type":"code","execution_count":107,"metadata":{"id":"e069c377","executionInfo":{"status":"ok","timestamp":1661635422227,"user_tz":-60,"elapsed":3,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["X = df['response']\n","y = df[label]"],"id":"e069c377"},{"cell_type":"code","execution_count":108,"metadata":{"id":"b02361ae","executionInfo":{"status":"ok","timestamp":1661635424245,"user_tz":-60,"elapsed":4,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)"],"id":"b02361ae"},{"cell_type":"code","execution_count":109,"metadata":{"id":"2e7de3f3","executionInfo":{"status":"ok","timestamp":1661635424245,"user_tz":-60,"elapsed":3,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["texts = list(X_train)\n","texts_test = list(X_test)\n","texts_val = list(X_val)"],"id":"2e7de3f3"},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1661635424245,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"65ccde77","outputId":"42139595-ff56-40e3-def9-bf97697ac17a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    871\n","0    824\n","1    673\n","Name: final_pred_modified, dtype: int64"]},"metadata":{},"execution_count":110}],"source":["df[label].value_counts()"],"id":"65ccde77"},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1661635424245,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"399a6c56","outputId":"648cb854-71ea-47f3-f243-08f749608d9b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["526     Are you aware of on-goings within your local c...\n","92      Congratulations! You have shown foresighted co...\n","963     I would like to emphasise that the purpose of ...\n","909     Congratulations! You have shown tender compass...\n","2242    This is a fantastic start but bear in mind tha...\n","                              ...                        \n","1033     As a first step : shall we continue with ESA?...\n","1731    I would like to emphasize that the purpose of ...\n","763     Think about how you plan to achieve this. When...\n","835                        How are you feeling right now?\n","1653    Let's talk a little bit more about you - Do yo...\n","Name: response, Length: 1657, dtype: object"]},"metadata":{},"execution_count":111}],"source":["X_train"],"id":"399a6c56"},{"cell_type":"markdown","metadata":{"id":"51f42f59"},"source":["## Model prediction"],"id":"51f42f59"},{"cell_type":"code","source":["%%capture\n","#!pip install transformer[sentencepiece]"],"metadata":{"id":"DyZ0ODbNtbYr","executionInfo":{"status":"ok","timestamp":1661635426366,"user_tz":-60,"elapsed":3,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"id":"DyZ0ODbNtbYr","execution_count":112,"outputs":[]},{"cell_type":"code","execution_count":114,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11727,"status":"ok","timestamp":1661635849310,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"fb96d763","outputId":"18b7de80-8dcb-4915-f734-32abf44578b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["\n","#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n","#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","#model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n","#tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","#model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n"],"id":"fb96d763"},{"cell_type":"code","execution_count":115,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1661635849310,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"5d65e814"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","class EmpatheticDataset(Dataset):\n","    def __init__(self, X, y, with_label=True):\n","        self.X = X.reset_index()['response']\n","        self.y = y.reset_index()[label]\n","        self.text = list(X.values)\n","        encoded_input = tokenizer(self.text, padding=True, truncation=True, return_tensors=\"pt\")\n","        encoded_input['text'] = self.text\n","        encoded_input['label'] = torch.tensor(y.values)\n","        if with_label:\n","            try:\n","                self.new_output = [{'label': label, 'text': text,'input_ids': input_ids, \\\n","                           'token_type_ids': token_type_ids, 'attention_mask': attention_mask} \\\n","                           for label, text, input_ids, token_type_ids, attention_mask in zip(encoded_input['label'], \\\n","                                                                              encoded_input['text'], \\\n","                                                                              encoded_input['input_ids'], \\\n","                                                                              encoded_input['token_type_ids'], \\\n","                                                                              encoded_input['attention_mask'])]\n","            except:\n","                self.new_output = [{'label': label, 'text': text,'input_ids': input_ids, \\\n","                           'attention_mask': attention_mask} \\\n","                           for label, text, input_ids, attention_mask in zip(encoded_input['label'], \\\n","                                                                              encoded_input['text'], \\\n","                                                                              encoded_input['input_ids'], \\\n","                                                                              encoded_input['attention_mask'])]\n","        else:\n","            try:\n","                self.new_output = [{'text': text,'input_ids': input_ids, \\\n","                           'token_type_ids': token_type_ids, 'attention_mask': attention_mask} \\\n","                           for text, input_ids, token_type_ids, attention_mask in zip(encoded_input['text'], \\\n","                                                                              encoded_input['input_ids'], \\\n","                                                                              encoded_input['token_type_ids'], \\\n","                                                                              encoded_input['attention_mask'])]\n","            except:\n","                self.new_output = [{'text': text,'input_ids': input_ids, \\\n","                           'attention_mask': attention_mask} \\\n","                           for text, input_ids, attention_mask in zip(encoded_input['text'], \\\n","                                                                              encoded_input['input_ids'], \\\n","                                                                              encoded_input['attention_mask'])]\n","            \n","        \n"," \n","    def __len__(self):\n","        return len(self.X)\n","   \n","    def __getitem__(self,idx):\n","        try:\n","            return self.new_output[idx]\n","        except:\n","            print(idx)\n","            print(self.new_output[idx - 1])\n","            print(self.new_output[idx])\n","        #return self.X[idx], self.y[idx]\n","        #features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask']"],"id":"5d65e814"},{"cell_type":"code","execution_count":116,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1661635849310,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"d6ba5539"},"outputs":[],"source":["train_data = EmpatheticDataset(X_train, y_train)\n","test_data = EmpatheticDataset(X_test, y_test)\n","val_data = EmpatheticDataset(X_val, y_val)"],"id":"d6ba5539"},{"cell_type":"code","execution_count":117,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1661635849311,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"4b72b3ea"},"outputs":[],"source":["#train_dataloader = DataLoader(train_data, shuffle=True, batch_size=8)\n","#test_dataloader = DataLoader(test_data, batch_size=8)"],"id":"4b72b3ea"},{"cell_type":"code","execution_count":118,"metadata":{"executionInfo":{"elapsed":2274,"status":"ok","timestamp":1661635851576,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"374eef94"},"outputs":[],"source":["from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")"],"id":"374eef94"},{"cell_type":"code","execution_count":119,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1661635851576,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"3b2b3667"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"],"id":"3b2b3667"},{"cell_type":"code","source":["def compute_metrics(p):\n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n","    precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n","    f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"],"metadata":{"id":"1_ycLKEk58uB","executionInfo":{"status":"ok","timestamp":1661635851576,"user_tz":-60,"elapsed":7,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"id":"1_ycLKEk58uB","execution_count":120,"outputs":[]},{"cell_type":"code","execution_count":121,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1661635851576,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"d24fbd0b","colab":{"base_uri":"https://localhost:8080/"},"outputId":"866356bb-2dd7-43ba-b923-0d6d9b635117"},"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", \n","                                  metric_for_best_model='accuracy',\n","                                  learning_rate=5e-5, #5e-5\n","                                  per_device_train_batch_size=8, \n","                                  per_device_eval_batch_size=8,\n","                                  adafactor=False,\n","                                  logging_first_step=True,\n","                                  logging_steps=100,\n","                                  num_train_epochs=5,\n","                                  adam_epsilon=1e-3)"],"id":"d24fbd0b"},{"cell_type":"code","execution_count":122,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1661635851577,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"05897353"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_data,\n","    eval_dataset=val_data,\n","    compute_metrics=compute_metrics,\n","    \n",")"],"id":"05897353"},{"cell_type":"code","execution_count":123,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":259366,"status":"ok","timestamp":1661636110936,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"ab0a475f","outputId":"60aa4fe2-51a4-4d7b-a7d0-bf0cdd93e714"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1657\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1040\n","The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1040/1040 04:19, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.936700</td>\n","      <td>0.877542</td>\n","      <td>0.580282</td>\n","      <td>0.580282</td>\n","      <td>0.580282</td>\n","      <td>0.580282</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.866600</td>\n","      <td>0.785818</td>\n","      <td>0.647887</td>\n","      <td>0.647887</td>\n","      <td>0.647887</td>\n","      <td>0.647887</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.740900</td>\n","      <td>0.755284</td>\n","      <td>0.656338</td>\n","      <td>0.656338</td>\n","      <td>0.656338</td>\n","      <td>0.656338</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.708600</td>\n","      <td>0.713397</td>\n","      <td>0.687324</td>\n","      <td>0.687324</td>\n","      <td>0.687324</td>\n","      <td>0.687324</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.712100</td>\n","      <td>0.706986</td>\n","      <td>0.690141</td>\n","      <td>0.690141</td>\n","      <td>0.690141</td>\n","      <td>0.690141</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 355\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 355\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","Saving model checkpoint to test_trainer/checkpoint-500\n","Configuration saved in test_trainer/checkpoint-500/config.json\n","Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 355\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 355\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","Saving model checkpoint to test_trainer/checkpoint-1000\n","Configuration saved in test_trainer/checkpoint-1000/config.json\n","Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 355\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1040, training_loss=0.8015521095349238, metrics={'train_runtime': 259.357, 'train_samples_per_second': 31.944, 'train_steps_per_second': 4.01, 'total_flos': 634383408624210.0, 'train_loss': 0.8015521095349238, 'epoch': 5.0})"]},"metadata":{},"execution_count":123}],"source":["trainer.train()"],"id":"ab0a475f"},{"cell_type":"code","execution_count":124,"metadata":{"id":"e01f447e","executionInfo":{"status":"ok","timestamp":1661636110936,"user_tz":-60,"elapsed":27,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["save_path = \"distilbert-base-uncased_full_data.pt\"\n","#torch.save(model.state_dict(), save_path)"],"id":"e01f447e"},{"cell_type":"code","execution_count":125,"metadata":{"id":"c531ed65","executionInfo":{"status":"ok","timestamp":1661636110936,"user_tz":-60,"elapsed":26,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"outputs":[],"source":["# METHOD 1\n","#save_path = \"hf_models/bert-base-uncased_full_data.pt\"\n","#model2 = torch.load(save_path)"],"id":"c531ed65"},{"cell_type":"code","source":["# METHOD 2\n","#trainer.save_model(\"/content/gdrive/MyDrive/Imperial College London/Individual Project/Model\")"],"metadata":{"id":"dZBtUcj88WlO","executionInfo":{"status":"ok","timestamp":1661636110937,"user_tz":-60,"elapsed":26,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"id":"dZBtUcj88WlO","execution_count":126,"outputs":[]},{"cell_type":"code","source":["# METHOD 3: SAVE in colab\n","#from google.colab import drive\n","#drive.mount('/content/gdrive')"],"metadata":{"id":"pyaaAQh5-vUU","executionInfo":{"status":"ok","timestamp":1661636110938,"user_tz":-60,"elapsed":27,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"id":"pyaaAQh5-vUU","execution_count":127,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"328ea14e"},"source":["## Loading Pre-trained model"],"id":"328ea14e"},{"cell_type":"code","execution_count":128,"metadata":{"id":"106d56a0","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1661636110938,"user_tz":-60,"elapsed":27,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}},"outputId":"2302f031-da04-462a-84ab-50f3c0453c1f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nmodel2 = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\\nmodel2.load_state_dict(torch.load(save_path, map_location=\"cpu\"))\\ntrainer2 = Trainer(\\n    model=model2,\\n    args=training_args,\\n    train_dataset=train_data,\\n    eval_dataset=val_data,\\n    compute_metrics=compute_metrics,\\n)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":128}],"source":["# METHOD 1\n","\"\"\"\n","\n","model2 = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n","model2.load_state_dict(torch.load(save_path, map_location=\"cpu\"))\n","trainer2 = Trainer(\n","    model=model2,\n","    args=training_args,\n","    train_dataset=train_data,\n","    eval_dataset=val_data,\n","    compute_metrics=compute_metrics,\n",")\n","\"\"\""],"id":"106d56a0"},{"cell_type":"code","source":["# METHOD 2\n","#model = AutoModelForSequenceClassification.from_pretrained(\"/content/gdrive/MyDrive/Imperial College London/Individual Project/Model\", local_files_only=True)\n","#trainer.model = model.cuda()"],"metadata":{"id":"T24BqL4P9BR0","executionInfo":{"status":"ok","timestamp":1661636110938,"user_tz":-60,"elapsed":25,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}}},"id":"T24BqL4P9BR0","execution_count":129,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89998768"},"source":["# Inference"],"id":"89998768"},{"cell_type":"code","execution_count":130,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"executionInfo":{"elapsed":3193,"status":"ok","timestamp":1661636114107,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"7bc0ab80","outputId":"4c809828-0d19-4db9-f910-7e0d73fd2d00"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 356\n","  Batch size = 8\n","The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}],"source":["y_pred_hf = trainer.predict(test_data)\n","y_pred_dl = np.argmax(y_pred_hf[0], axis=1)"],"id":"7bc0ab80"},{"cell_type":"code","execution_count":131,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1661636114107,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"},"user_tz":-60},"id":"a616761a","outputId":"7e62f1f3-66a2-4d06-8ef5-db789bc2c015"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.83      0.83       127\n","           1       0.56      0.30      0.40       105\n","           2       0.64      0.88      0.74       124\n","\n","    accuracy                           0.69       356\n","   macro avg       0.67      0.67      0.65       356\n","weighted avg       0.68      0.69      0.67       356\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_pred_dl))"],"id":"a616761a"},{"cell_type":"markdown","source":["## Time spent on inference\n"],"metadata":{"id":"OVXRGJZP9rIJ"},"id":"OVXRGJZP9rIJ"},{"cell_type":"code","source":["X_time_discard, X_time, y_time_discard, y_time= train_test_split(X, y, test_size=0.01, random_state=0)"],"metadata":{"id":"bJnUMnj_l_FH"},"id":"bJnUMnj_l_FH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_data = EmpatheticDataset(X_time, y_time)"],"metadata":{"id":"8eklmNtI7MKF"},"id":"8eklmNtI7MKF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bf0182d","executionInfo":{"status":"ok","timestamp":1660575369431,"user_tz":-60,"elapsed":10,"user":{"displayName":"Wei Jie Chua","userId":"08262770765452861591"}},"colab":{"base_uri":"https://localhost:8080/","height":144},"outputId":"7cb6da93-7302-448b-f389-9ee61b729c27"},"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 24\n","  Batch size = 8\n","The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[""]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Time spent: 0.14568734169006348\n","Inference data size: 24\n"]}],"source":["import time\n","\n","start = time.time()\n","y_pred_hf = trainer.predict(time_data)\n","y_pred_dl = np.argmax(y_pred_hf[0], axis=1)\n","end = time.time()\n","time_spent = end - start\n","print(f\"Time spent: {time_spent}\")\n","print(f\"Inference data size: {len(time_data)}\")"],"id":"0bf0182d"},{"cell_type":"code","source":[],"metadata":{"id":"bA-cS3zK7hLj"},"id":"bA-cS3zK7hLj","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"Prediction.ipynb","provenance":[],"collapsed_sections":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}