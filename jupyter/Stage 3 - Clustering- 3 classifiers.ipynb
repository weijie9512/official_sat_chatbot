{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9d7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cite simcse\n",
    "# cite sentence-transformers https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239e25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26746fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_own = pd.read_excel(\"data/survey_data/labelled_data/sat_data_combined_vertical.xlsx\")\n",
    "df_own = pd.read_excel(\"../data/survey_data/labelled_data/data_labelled.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc08746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_own = df_own[df_own['empathy_score'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d53f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>response</th>\n",
       "      <th>final_pred_modified</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>How do you feel?</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling?</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling today?</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling right now?</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>763</td>\n",
       "      <td>J05</td>\n",
       "      <td>Share a blog post about your success story, di...</td>\n",
       "      <td>0</td>\n",
       "      <td>paraphrased2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>764</td>\n",
       "      <td>J05</td>\n",
       "      <td>Write a blog post sharing your success story (...</td>\n",
       "      <td>0</td>\n",
       "      <td>paraphrased2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>765</td>\n",
       "      <td>J05</td>\n",
       "      <td>Let us write a blog post discussing your succe...</td>\n",
       "      <td>2</td>\n",
       "      <td>paraphrased2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>766</td>\n",
       "      <td>J05</td>\n",
       "      <td>Make a blog post recording your success story,...</td>\n",
       "      <td>0</td>\n",
       "      <td>paraphrased2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>767</td>\n",
       "      <td>J05</td>\n",
       "      <td>You can write a blog post sharing your success...</td>\n",
       "      <td>0</td>\n",
       "      <td>paraphrased2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2368 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 level_1                                           response  \\\n",
       "0              0     A01                                   How do you feel?   \n",
       "1              1     A01                               How are you feeling?   \n",
       "2              2     A01                         How are you feeling today?   \n",
       "3              3     A01                                 How are you doing?   \n",
       "4              4     A01                     How are you feeling right now?   \n",
       "...          ...     ...                                                ...   \n",
       "2363         763     J05  Share a blog post about your success story, di...   \n",
       "2364         764     J05  Write a blog post sharing your success story (...   \n",
       "2365         765     J05  Let us write a blog post discussing your succe...   \n",
       "2366         766     J05  Make a blog post recording your success story,...   \n",
       "2367         767     J05  You can write a blog post sharing your success...   \n",
       "\n",
       "      final_pred_modified      original  \n",
       "0                       0      original  \n",
       "1                       0      original  \n",
       "2                       0      original  \n",
       "3                       0      original  \n",
       "4                       0      original  \n",
       "...                   ...           ...  \n",
       "2363                    0  paraphrased2  \n",
       "2364                    0  paraphrased2  \n",
       "2365                    2  paraphrased2  \n",
       "2366                    0  paraphrased2  \n",
       "2367                    0  paraphrased2  \n",
       "\n",
       "[2368 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4a63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['empathy_score'].replace(1, 0, inplace=True)\n",
    "#df['empathy_score'].replace(2, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f8d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_response = df_own['response'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656dd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5bd8e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    871\n",
       "0    824\n",
       "1    673\n",
       "Name: final_pred_modified, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['final_pred_modified'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0326f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['response']\n",
    "y = df['final_pred_modified']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695e334",
   "metadata": {},
   "source": [
    "## Lisa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2831ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/compassion_energy/empathy_labelled_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be9110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['empathy_score'] = df['empathy_score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "060f250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['response']\n",
    "y = df['empathy_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a51c5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    388\n",
       "2    381\n",
       "0    331\n",
       "Name: empathy_score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['empathy_score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77560d5f",
   "metadata": {},
   "source": [
    "## Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ae22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c647155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(X_train)\n",
    "texts_test = list(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198145f",
   "metadata": {},
   "source": [
    "### SimCSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ce7ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa49b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9951ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our models. The package will take care of downloading the models automatically\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\")\n",
    "#model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-roberta-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e904c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "451dcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = tokenizer(texts_test, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings_test = model(**inputs_test, output_hidden_states=True, return_dict=True).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0ec3dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25176a32",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "986f66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(embeddings, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad001be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8cb3a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.47      0.54       302\n",
      "           1       0.46      0.64      0.54       342\n",
      "           2       0.79      0.67      0.72       346\n",
      "\n",
      "    accuracy                           0.60       990\n",
      "   macro avg       0.63      0.59      0.60       990\n",
      "weighted avg       0.63      0.60      0.60       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c18d48",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7df10d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "193bb9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;,\n",
       "              predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b17d1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb.predict(embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2962576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.41      0.48       302\n",
      "           1       0.43      0.62      0.51       342\n",
      "           2       0.77      0.62      0.69       346\n",
      "\n",
      "    accuracy                           0.56       990\n",
      "   macro avg       0.59      0.55      0.56       990\n",
      "weighted avg       0.59      0.56      0.56       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55034935",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "300d00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(embeddings, y_train)\n",
    "y_pred = gnb.predict(embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0f61767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.49      0.50       302\n",
      "           1       0.42      0.45      0.43       342\n",
      "           2       0.72      0.68      0.70       346\n",
      "\n",
      "    accuracy                           0.54       990\n",
      "   macro avg       0.55      0.54      0.54       990\n",
      "weighted avg       0.55      0.54      0.55       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c6111",
   "metadata": {},
   "source": [
    "## Write to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddaeafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "f5f486ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_own_logit = clf.predict(embeddings_own)\n",
    "y_pred_own_xgb = xgb.predict(embeddings_own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "99570771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "fd662298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own['y_pred_logit'] = y_pred_own_logit\n",
    "df_own['y_pred_xgb'] = y_pred_own_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "80781e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_own.to_excel(\"data/survey_data/labelled_data/sat_data_combined_vertical_pred.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "15fa99c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    355\n",
       "2    311\n",
       "1    166\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_own['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f028f4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    388\n",
       "2    381\n",
       "0    331\n",
       "Name: empathy_score, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['empathy_score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f42f59",
   "metadata": {},
   "source": [
    "## Train One more layer in front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "fb96d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bigscience/bloom-2b5/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /Users/weijiechua/.cache/huggingface/transformers/tmp30ishouw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d34c117f6e4e9e90d3c37066916abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bigscience/bloom-2b5/resolve/main/tokenizer_config.json in cache at /Users/weijiechua/.cache/huggingface/transformers/5b4f89682f2cc48a2f3c7a76f23268acee5a2dd6fd91742b39c6ecde0f70a29e.965934b32e58e96d4f4145b1bcd96eabcb838a46d2cfcbbd241721ef294d461b\n",
      "creating metadata file for /Users/weijiechua/.cache/huggingface/transformers/5b4f89682f2cc48a2f3c7a76f23268acee5a2dd6fd91742b39c6ecde0f70a29e.965934b32e58e96d4f4145b1bcd96eabcb838a46d2cfcbbd241721ef294d461b\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class BloomTokenizerFast does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [628]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", num_labels=3)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigscience/bloom-2b5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigscience/bloom-2b5\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/gen/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:552\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer class BloomTokenizerFast does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", num_labels=3)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-2b5\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"bigscience/bloom-2b5\", num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "5d65e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class EmpatheticDataset(Dataset):\n",
    "    def __init__(self, X, y, with_label=True):\n",
    "        self.X = X.reset_index()['response']\n",
    "        self.y = y.reset_index()['empathy_score']\n",
    "        self.text = list(X.values)\n",
    "        encoded_input = tokenizer(self.text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_input['text'] = self.text\n",
    "        encoded_input['label'] = torch.tensor(y.values)\n",
    "        if with_label:\n",
    "            try:\n",
    "                self.new_output = [{'label': label, 'text': text,'input_ids': input_ids, \\\n",
    "                           'token_type_ids': token_type_ids, 'attention_mask': attention_mask} \\\n",
    "                           for label, text, input_ids, token_type_ids, attention_mask in zip(encoded_input['label'], \\\n",
    "                                                                              encoded_input['text'], \\\n",
    "                                                                              encoded_input['input_ids'], \\\n",
    "                                                                              encoded_input['token_type_ids'], \\\n",
    "                                                                              encoded_input['attention_mask'])]\n",
    "            except:\n",
    "                self.new_output = [{'label': label, 'text': text,'input_ids': input_ids, \\\n",
    "                           'attention_mask': attention_mask} \\\n",
    "                           for label, text, input_ids, attention_mask in zip(encoded_input['label'], \\\n",
    "                                                                              encoded_input['text'], \\\n",
    "                                                                              encoded_input['input_ids'], \\\n",
    "                                                                              encoded_input['attention_mask'])]\n",
    "        else:\n",
    "            try:\n",
    "                self.new_output = [{'text': text,'input_ids': input_ids, \\\n",
    "                           'token_type_ids': token_type_ids, 'attention_mask': attention_mask} \\\n",
    "                           for text, input_ids, token_type_ids, attention_mask in zip(encoded_input['text'], \\\n",
    "                                                                              encoded_input['input_ids'], \\\n",
    "                                                                              encoded_input['token_type_ids'], \\\n",
    "                                                                              encoded_input['attention_mask'])]\n",
    "            except:\n",
    "                self.new_output = [{'text': text,'input_ids': input_ids, \\\n",
    "                           'attention_mask': attention_mask} \\\n",
    "                           for text, input_ids, attention_mask in zip(encoded_input['text'], \\\n",
    "                                                                              encoded_input['input_ids'], \\\n",
    "                                                                              encoded_input['attention_mask'])]\n",
    "            \n",
    "        \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        try:\n",
    "            return self.new_output[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            print(self.new_output[idx - 1])\n",
    "            print(self.new_output[idx])\n",
    "        #return self.X[idx], self.y[idx]\n",
    "        #features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "dae57950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_input = tokenizer(list(X_train.values), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#a = [{\"input_ids\": k, \"attention_masks\":v} for k,v in zip(encoded_input['input_ids'], encoded_input['attention_mask'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "d6ba5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EmpatheticDataset(X_train, y_train)\n",
    "test_data = EmpatheticDataset(X_test, y_test)\n",
    "own_data = EmpatheticDataset(own_X, own_y, with_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "4b72b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataloader = DataLoader(train_data, shuffle=True, batch_size=8)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "964555aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "374eef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "3b2b3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "d24fbd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "05897353",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "ab0a475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weijiechua/miniforge3/envs/gen/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 770\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 291\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291/291 04:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672841</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596894</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573647</td>\n",
       "      <td>0.793939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 330\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 330\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 330\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=291, training_loss=0.5550552840085373, metrics={'train_runtime': 279.2318, 'train_samples_per_second': 8.273, 'train_steps_per_second': 1.042, 'total_flos': 80722374329520.0, 'train_loss': 0.5550552840085373, 'epoch': 3.0})"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "ee6f1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"bert-base-uncased.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "6e05e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b036668",
   "metadata": {},
   "source": [
    "# toCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "468bd75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1536\n",
      "  Batch size = 8\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    }
   ],
   "source": [
    "y_pred_hf = trainer.predict(own_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "939dffb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 2.7150385 , -0.6916882 , -2.6574807 ],\n",
       "       [ 2.812517  , -0.8007507 , -2.758556  ],\n",
       "       [ 2.7168624 , -0.66955656, -2.5950184 ],\n",
       "       ...,\n",
       "       [-2.49317   , -0.27664855,  3.0948815 ],\n",
       "       [-2.3698714 , -0.06933565,  2.8442252 ],\n",
       "       [-2.2787018 ,  0.17882192,  2.5538347 ]], dtype=float32), label_ids=None, metrics={'test_runtime': 63.4253, 'test_samples_per_second': 24.217, 'test_steps_per_second': 3.027})"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "f59c7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl = np.argmax(y_pred_hf[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "4a3cbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "2f83cc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "a9f5c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own['y_pred_bert'] = y_pred_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "958a4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_own.to_excel(\"data/survey_data/labelled_data/sat_data_combined_vertical_pred.xlsx\")\n",
    "df_own.to_excel(\"data/survey_data/labelled_data/data_pm_combined_vertical_pred.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bc27c",
   "metadata": {},
   "source": [
    "## Fine tune on SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "6392367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /Users/weijiechua/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/vocab.txt from cache at /Users/weijiechua/.cache/huggingface/transformers/09102786ff74bdc2d32e48fb8505b1d86fd33b33c1e1f149322505c3fcc8926e.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/special_tokens_map.json from cache at /Users/weijiechua/.cache/huggingface/transformers/8c406286308d13c3a53bc10c3d1a2d5113d4e46a34cb6ec5ee06e5d9762c462c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/weijiechua/.cache/huggingface/transformers/4f2880ff62576ab971eea56ed4efbe8766ec79f9b35011e7ee8260a7feb608b8.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /Users/weijiechua/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /Users/weijiechua/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at /Users/weijiechua/.cache/huggingface/transformers/886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/weijiechua/.cache/huggingface/transformers/4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Import our models. The package will take care of downloading the models automatically\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "c543cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class EmpatheticDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.reset_index()['response']\n",
    "        self.y = y.reset_index()['empathy_score']\n",
    "        self.text = list(X.values)\n",
    "        encoded_input = tokenizer(self.text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_input['text'] = self.text\n",
    "        encoded_input['label'] = torch.tensor(y.values)\n",
    "        self.new_output = [{'label': label, 'text': text,'input_ids': input_ids, \\\n",
    "                       'token_type_ids': token_type_ids, 'attention_mask': attention_mask} \\\n",
    "                       for label, text, input_ids, token_type_ids, attention_mask in zip(encoded_input['label'], \\\n",
    "                                                                          encoded_input['text'], \\\n",
    "                                                                          encoded_input['input_ids'], \\\n",
    "                                                                          encoded_input['token_type_ids'], \\\n",
    "                                                                          encoded_input['attention_mask'])]\n",
    "        \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        try:\n",
    "            return self.new_output[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            print(self.new_output[idx - 1])\n",
    "            print(self.new_output[idx])\n",
    "        #return self.X[idx], self.y[idx]\n",
    "        #features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3b671419",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EmpatheticDataset(X_train, y_train)\n",
    "test_data = EmpatheticDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a86f7a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6ccf6",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2da1ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis = pd.read_excel(\"data/survey_data/labelled_data/sat_data_combined_vertical_pred.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1dea0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vis['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "bb7f1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['response_len'] = df_vis['response'].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "07eeb587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>i</th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>response</th>\n",
       "      <th>empathy_score</th>\n",
       "      <th>y_pred_logit</th>\n",
       "      <th>y_pred_xgb</th>\n",
       "      <th>response_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>How do you feel?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling today?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling right now?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>827</td>\n",
       "      <td>575</td>\n",
       "      <td>8</td>\n",
       "      <td>J05</td>\n",
       "      <td>Write down a blog post sharing your success st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>828</td>\n",
       "      <td>639</td>\n",
       "      <td>9</td>\n",
       "      <td>J05</td>\n",
       "      <td>Write a blog post sharing your success story. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>829</td>\n",
       "      <td>703</td>\n",
       "      <td>10</td>\n",
       "      <td>J05</td>\n",
       "      <td>Let us write down a blog post discussing your ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>830</td>\n",
       "      <td>767</td>\n",
       "      <td>11</td>\n",
       "      <td>J05</td>\n",
       "      <td>Make a blog post recording your success story....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>831</td>\n",
       "      <td>831</td>\n",
       "      <td>12</td>\n",
       "      <td>J05</td>\n",
       "      <td>You can write a blog post sharing your success...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>832 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    i  level_0 level_1  \\\n",
       "0             0    0        0     A01   \n",
       "1             1   64        1     A01   \n",
       "2             2  128        2     A01   \n",
       "3             3  192        3     A01   \n",
       "4             4  256        4     A01   \n",
       "..          ...  ...      ...     ...   \n",
       "827         827  575        8     J05   \n",
       "828         828  639        9     J05   \n",
       "829         829  703       10     J05   \n",
       "830         830  767       11     J05   \n",
       "831         831  831       12     J05   \n",
       "\n",
       "                                              response  empathy_score  \\\n",
       "0                                     How do you feel?            1.0   \n",
       "1                                 How are you feeling?            1.0   \n",
       "2                           How are you feeling today?            1.0   \n",
       "3                                   How are you doing?            1.0   \n",
       "4                       How are you feeling right now?            1.0   \n",
       "..                                                 ...            ...   \n",
       "827  Write down a blog post sharing your success st...            NaN   \n",
       "828  Write a blog post sharing your success story. ...            NaN   \n",
       "829  Let us write down a blog post discussing your ...            NaN   \n",
       "830  Make a blog post recording your success story....            NaN   \n",
       "831  You can write a blog post sharing your success...            NaN   \n",
       "\n",
       "     y_pred_logit  y_pred_xgb  response_len  \n",
       "0               0           0             4  \n",
       "1               0           0             4  \n",
       "2               0           0             5  \n",
       "3               0           0             4  \n",
       "4               0           0             6  \n",
       "..            ...         ...           ...  \n",
       "827             0           0            30  \n",
       "828             0           1            24  \n",
       "829             2           2            40  \n",
       "830             0           0            36  \n",
       "831             2           2            36  \n",
       "\n",
       "[832 rows x 9 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "80280b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['y_pred_avg'] = (df_vis['y_pred_logit'] + df_vis['y_pred_xgb'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4bb5c660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>i</th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>response</th>\n",
       "      <th>empathy_score</th>\n",
       "      <th>y_pred_logit</th>\n",
       "      <th>y_pred_xgb</th>\n",
       "      <th>response_len</th>\n",
       "      <th>y_pred_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>How do you feel?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling today?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you doing?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>A01</td>\n",
       "      <td>How are you feeling right now?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>827</td>\n",
       "      <td>575</td>\n",
       "      <td>8</td>\n",
       "      <td>J05</td>\n",
       "      <td>Write down a blog post sharing your success st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>828</td>\n",
       "      <td>639</td>\n",
       "      <td>9</td>\n",
       "      <td>J05</td>\n",
       "      <td>Write a blog post sharing your success story. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>829</td>\n",
       "      <td>703</td>\n",
       "      <td>10</td>\n",
       "      <td>J05</td>\n",
       "      <td>Let us write down a blog post discussing your ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>830</td>\n",
       "      <td>767</td>\n",
       "      <td>11</td>\n",
       "      <td>J05</td>\n",
       "      <td>Make a blog post recording your success story....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>831</td>\n",
       "      <td>831</td>\n",
       "      <td>12</td>\n",
       "      <td>J05</td>\n",
       "      <td>You can write a blog post sharing your success...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>832 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    i  level_0 level_1  \\\n",
       "0             0    0        0     A01   \n",
       "1             1   64        1     A01   \n",
       "2             2  128        2     A01   \n",
       "3             3  192        3     A01   \n",
       "4             4  256        4     A01   \n",
       "..          ...  ...      ...     ...   \n",
       "827         827  575        8     J05   \n",
       "828         828  639        9     J05   \n",
       "829         829  703       10     J05   \n",
       "830         830  767       11     J05   \n",
       "831         831  831       12     J05   \n",
       "\n",
       "                                              response  empathy_score  \\\n",
       "0                                     How do you feel?            1.0   \n",
       "1                                 How are you feeling?            1.0   \n",
       "2                           How are you feeling today?            1.0   \n",
       "3                                   How are you doing?            1.0   \n",
       "4                       How are you feeling right now?            1.0   \n",
       "..                                                 ...            ...   \n",
       "827  Write down a blog post sharing your success st...            NaN   \n",
       "828  Write a blog post sharing your success story. ...            NaN   \n",
       "829  Let us write down a blog post discussing your ...            NaN   \n",
       "830  Make a blog post recording your success story....            NaN   \n",
       "831  You can write a blog post sharing your success...            NaN   \n",
       "\n",
       "     y_pred_logit  y_pred_xgb  response_len  y_pred_avg  \n",
       "0               0           0             4         0.0  \n",
       "1               0           0             4         0.0  \n",
       "2               0           0             5         0.0  \n",
       "3               0           0             4         0.0  \n",
       "4               0           0             6         0.0  \n",
       "..            ...         ...           ...         ...  \n",
       "827             0           0            30         0.0  \n",
       "828             0           1            24         0.5  \n",
       "829             2           2            40         2.0  \n",
       "830             0           0            36         0.0  \n",
       "831             2           2            36         2.0  \n",
       "\n",
       "[832 rows x 10 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4eb82606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['bin'] = pd.qcut(df_vis['response_len'], 5, labels=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7a2bc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis_line = df_vis[['bin', 'y_pred_avg']].groupby(['bin']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "cc26ca97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.655844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.105590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.517341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.835443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_pred_avg\n",
       "bin            \n",
       "0      0.150538\n",
       "1      0.655844\n",
       "2      1.105590\n",
       "3      1.517341\n",
       "4      1.835443"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1274e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis_line = df_vis_line.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "908351ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis_line.columns = ['bin', 'y_pred_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c47480fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "Name: bin, dtype: category\n",
       "Categories (5, int64): [0 < 1 < 2 < 3 < 4]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis_line['bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b4e0599a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='bin', ylabel='y_pred_avg'>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(data=df_vis_line, x=\"bin\", y=\"y_pred_avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc8642",
   "metadata": {},
   "source": [
    "## Lisa Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "fb9a24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis = pd.read_csv(\"data/compassion_energy/empathy_labelled_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "21081450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['response_len'] = df_vis['response'].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e14d6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['bin'] = pd.qcut(df_vis['response_len'], 5, labels=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5e09c176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>annotator1_score</th>\n",
       "      <th>annotator2_score</th>\n",
       "      <th>annotator3_score</th>\n",
       "      <th>empathy_score</th>\n",
       "      <th>response_len</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When you've selected the protocol you want to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did something happen to you?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you for taking part, I really appreciate...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am grateful for your participation. I hope t...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you for being so open, I'm trying to det...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  annotator1_score  \\\n",
       "0  When you've selected the protocol you want to ...                 1   \n",
       "1                       Did something happen to you?                 0   \n",
       "2  Thank you for taking part, I really appreciate...                 2   \n",
       "3  I am grateful for your participation. I hope t...                 1   \n",
       "4  Thank you for being so open, I'm trying to det...                 2   \n",
       "\n",
       "   annotator2_score  annotator3_score  empathy_score  response_len bin  \n",
       "0                 2                 2              2            24   3  \n",
       "1                 0                 1              0             5   0  \n",
       "2                 1                 2              2            19   2  \n",
       "3                 2                 2              2            14   1  \n",
       "4                 2                 2              2            37   4  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "862dde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis_line = df_vis[['bin', 'response_len', 'empathy_score']].groupby(['bin']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "7a00cde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_len</th>\n",
       "      <th>empathy_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.660584</td>\n",
       "      <td>0.321168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.021858</td>\n",
       "      <td>0.677596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.322314</td>\n",
       "      <td>1.070248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.495146</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.989744</td>\n",
       "      <td>1.897436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     response_len  empathy_score\n",
       "bin                             \n",
       "0        9.660584       0.321168\n",
       "1       14.021858       0.677596\n",
       "2       17.322314       1.070248\n",
       "3       22.495146       1.500000\n",
       "4       32.989744       1.897436"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
